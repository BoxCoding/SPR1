{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"import plotly.graph_objs as go\\nimport chart_studio.plotly as py\\nimport plotly.offline as offline\\n#offline.init_notebook_mode()\\nimport cufflinks as cf\\npd.options.display.max_columns = 30\\nfrom IPython.core.interactiveshell import InteractiveShell\\nimport plotly.figure_factory as ff\\nInteractiveShell.ast_node_interactivity = 'all'\\nfrom plotly.offline import iplot\\ncf.go_offline()\\ncf.set_config_file(world_readable=True, theme='solar')\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pandas.io.common import is_url\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "import random\n",
    "from fuzzywuzzy import fuzz\n",
    "\"\"\"import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import plotly.offline as offline\n",
    "#offline.init_notebook_mode()\n",
    "import cufflinks as cf\n",
    "pd.options.display.max_columns = 30\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "cf.go_offline()\n",
    "cf.set_config_file(world_readable=True, theme='solar')\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:/scrapping/magicbricks/\"\n",
    "df=pd.read_csv(f'{path}data_noida_feb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"8 Bath,Semi-Furnished A Name That Epitomizes Strength & UniquenessThe Logo Focussed on Greenery, Vaastu And Premium Value. The logo has been designed to capture four things mainly: greenery, lots of sunlight, East facing and spaciousness. Besides all these, it has also been designed to capture unfettered, uncluttered living.The tagline also differentiates this property from other such properties in the market by its conceitedness. It reflects confidence of a premium property and the class of people owing such a property.'Sun Twilight' is the name of Project launched by under Company name 'Sunrise Structures & Developers Pvt. Ltd.' The project location is 'Sector 27, Opposite Delta 1 Metro Station, Greater Noida on 105 meter road and other side is 60 meter road. Project is next to City Park, Greater Noida and surrounded by best Commercial Market and Residential Societies.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.description[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have ', len(df), 'properties in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_description(society):\n",
    "    example = df[df.Society == society][['description', 'name']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Name:', example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_description('Mahagun Mantra 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Society.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description']=df['description'].replace('is_url','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(df['description'], 20)\n",
    "df1 = pd.DataFrame(common_words, columns = ['description' , 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  df['description'].str.contains('link').any():\n",
    "    print(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description']=df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby('description').sum()['count'].sort_values().iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in property description before removing stop words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(1, 3)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(df['description'], 20)\n",
    "df5 = pd.DataFrame(common_words, columns = ['description' , 'count'])\n",
    "df5.groupby('description').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description before removing stop words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '10', '20', '19', '15', 'Ground', '21', '7', '14', '23', '1',\n",
       "       '9', '5', '12', '3', '13', '2', '8', '4', '16', '18', '11', '22',\n",
       "       '30', '17', '6', '24', '28', '25', 'Upper Basement', '27', '29',\n",
       "       '31', '26'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.floorno.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sun Twilight', 'Purvanchal Royal City', 'ATS Destinaire',\n",
       "       'SKA Metro Ville', 'Arihant Ambar', 'Gaur City',\n",
       "       'Gaur Yamuna City', 'Prideville ', 'Page Three Residences',\n",
       "       'Stellar One', 'Rise Resort Residences', 'Ace Platinum',\n",
       "       'Eros Sampoornam I', 'Gaur Saundaryam', 'SKA Greenarch',\n",
       "       'Godrej Golf Links', 'ATS Dolce', 'Migsun Ultimo',\n",
       "       'Sikka Kaamya Greens', 'Gaurs Runway Suites', 'Meridian Towers',\n",
       "       'Jaypee Greens Villas', 'Paramount Golf Foreste', 'Raksha Addela',\n",
       "       'Gaur Atulyam', 'Czar Suites', 'Royal Apartments',\n",
       "       'Supertech Czar Villas', 'ATS Green Paradiso', nan,\n",
       "       'Supertech Up Country', 'Nimbus Express Park View II',\n",
       "       'Technocity Apartments', 'Plumeria Garden Estate',\n",
       "       'Harmukh Apartment', 'NRI City Township', 'Nirala Greenshire',\n",
       "       'Assotech Springfields', 'The Hemisphere', 'Mahagun Mantra 2',\n",
       "       'Ajnara Homes', 'Patel Neotown', 'NRI Residency',\n",
       "       'Ajnara Le Garden', 'Ajnara Panorama',\n",
       "       'Nimbus Express Park View I', 'Omaxe Palm Greens',\n",
       "       'Eldeco Mystic Greens', 'Project Unitech Horizon',\n",
       "       'Unitech Cascade', 'Vrinda City', 'Express Park view I',\n",
       "       'Eros Sampoornam', 'Mahagun Mantra 1', 'Unitech Horizon',\n",
       "       'Migsun Vilaasa', 'AVJ Heights', 'Victoryone Amara', 'Sportshome',\n",
       "       'Hawelia Valenova Park', 'Sublimis', 'ABA Corp Cherry County',\n",
       "       'Gaur City II 10TH Avenue', 'Arihant Abode', 'Stellar Jeevan',\n",
       "       'Jaypee Greens Moon Court', 'Jaypee Greens Spa Court',\n",
       "       'Purvanchal Silver City 2', 'Jaypee Greens Crescent Court',\n",
       "       'Krishna Homes', 'Novel Valley', 'Exotica Dreamville Arcade',\n",
       "       'Eldeco Residency Green Medows', 'Purvanchal Heights',\n",
       "       'Nirala Estate', '6th Avenue', 'Valencia Homes',\n",
       "       'Panchsheel Hynish', 'Defence Empire', 'The Villa Royale',\n",
       "       'Jaypee Greens Town Homes', 'Omaxe Connaught Place',\n",
       "       'Samridhi Grand Avenue Iconic Towers', 'Gaur City 1st Avenue',\n",
       "       'Designarch eHomes', 'Shivalik Homes', 'Gulshan Bellina',\n",
       "       'Golfforeste AC Apartments', 'Gaur Saundaryam Phase 2',\n",
       "       'AWHO Gurjinder Vihar', 'Ajnara London Square',\n",
       "       'Gaur City 5th Avenue', 'Galaxy Royale', 'Gaur City  7th Avenue',\n",
       "       'AIG Park Avenue', 'Project Parsvnath Estate', 'Park Lotus',\n",
       "       'Panchsheel Greens 2', 'Oasis Venetia Heights',\n",
       "       'Gaur City 2 White Orchid', 'Authority Plots',\n",
       "       'Stellar MI Citi Homes', 'Migsun Wynn', 'Himalaya Pride',\n",
       "       'AVJ Homes', 'Shri Radha Sky Garden', 'Parsvnath Platinum Floors',\n",
       "       'Imperia Residency', 'The Kings Reserve',\n",
       "       'Supertech Eco Village 3', 'ATS Allure', 'La Galaxia',\n",
       "       'Rudra Aqua Casa', 'Ajnara Homes Phase II', 'Parsvnath Panorama',\n",
       "       'Stellar Icon', 'Gaur City Center', 'Defence Enclave',\n",
       "       'Sushant Serene Residency', 'Airwil Green Avenue',\n",
       "       'Eldeco Citadel', 'Amrapali Golf Homes', 'Beetle Lap',\n",
       "       'Capital Athena', 'Mahaluxmi Green Mansion', 'Presidency Heights',\n",
       "       'Morpheus Pratiksha', 'Ansar Ashiyan', 'Amrapali Grand',\n",
       "       'Aarcity Regency Park', 'Unitech Verve', 'Ratan Pearls',\n",
       "       'The Western Star', 'Aims Green Avenue', 'Sanskriti',\n",
       "       'Jaypee Earth Court', 'Kings Park', 'RG Luxury Homes',\n",
       "       'Unibera Homes', 'Ska Green Mansion', 'Oasis My Homes',\n",
       "       'Imperia H2O', 'Yamuna Expressway Authority Plot', 'Laxmi Homes',\n",
       "       'AWHO Township', 'Shivalik Homes 2', 'Omaxe ITC', 'Alpha Homes',\n",
       "       'Grand Circuit', 'Noida Authority RWA', 'Parsvnath Palacia',\n",
       "       'HBA Tech Zone Residency', 'RST Galleria', 'City Square',\n",
       "       'Chi Phi', 'Eldeco Riviera', 'Venetia Heights',\n",
       "       'Stellar Sigma Apartments', 'NRI City Center', 'Mahagun Mywoods',\n",
       "       'Palm Olympia', 'Royal Nest', 'Fusion Homes', 'Lotus Villas',\n",
       "       'Devika Gold Homz', 'Paramount Golfmart', 'Vihaan Heritage',\n",
       "       'Gayatri Aura', 'Fairway Apartments', 'Unitech Heights',\n",
       "       'Urbainia Grid1', 'ATS Rhapsody'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Society.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(df['description'], 40)\n",
    "df6 = pd.DataFrame(common_words, columns = ['description' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['description'].apply(lambda x: len(str(x).split()))\n",
    "desc_lengths = list(df['word_count'])\n",
    "print(\"Number of descriptions:\",len(desc_lengths),\n",
    "      \"\\nAverage word count\", np.average(desc_lengths),\n",
    "      \"\\nMinimum word count\", min(desc_lengths),\n",
    "      \"\\nMaximum word count\", max(desc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'].iplot(\n",
    "    kind='hist',\n",
    "    bins = 50,\n",
    "    linecolor='black',\n",
    "    xTitle='word count',\n",
    "    yTitle='count',\n",
    "    title='Word Count Distribution in Hotel Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "url_symbol=re.compile('[^https?:\\/\\/.*[\\r\\n]*]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text =url_symbol.sub('',text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['desc_clean'] = df['description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(df['desc_clean'], 20)\n",
    "df6 = pd.DataFrame(common_words, columns = ['desc_clean' , 'count'])\n",
    "df6.groupby('desc_clean').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description after removing stop words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('name', inplace = True)\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(df['desc_clean'])\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(name, cosine_similarities = cosine_similarities):\n",
    "    \n",
    "    recommended_properties = []\n",
    "    \n",
    "    # gettin the index of the hotel that matches the name\n",
    "    if indices[indices == name].shape[0]>0:\n",
    "        idx = indices[indices == name].index[0]\n",
    "    else:\n",
    "        idx=clean_text(name)\n",
    "        tfidf_matrix = tf.fit_transform(idx)\n",
    "    print(idx)\n",
    "\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indexes of the 10 most similar hotels except itself\n",
    "    top_20_indexes = list(score_series.iloc[1:21].index)\n",
    "    \n",
    "    # populating the list with the names of the top 10 matching hotels\n",
    "    for i in top_20_indexes:\n",
    "        recommended_properties.append(list(df.index)[i])\n",
    "        \n",
    "    return recommended_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations('2 BHK Apartment for Sale in Shri Radha Sky Garden, Noida Extension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "count_vector = count.fit(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = count.fit_transform(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(reqbased,features):\n",
    "    indices = pd.Series(reqbased.index)\n",
    "    score_ind=0\n",
    "    index_ind=0\n",
    "    #print(reqbased.head())\n",
    "    #print(features)\n",
    "    for index,row  in reqbased.iterrows():\n",
    "        desc=row['projectName']\n",
    "        #print(desc)\n",
    "        score=fuzz.token_set_ratio(desc, features)\n",
    "        print(score)\n",
    "        if score>score_ind:\n",
    "            score_ind=score\n",
    "            index_ind=index\n",
    "    print(index_ind)\n",
    "    print(score_ind)\n",
    "    return reqbased[reqbased.index==index_ind].index[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommedation_budget(locality,budget=None,*features):\n",
    "    #print\n",
    "    df['locality']=df['locality'].str.lower()\n",
    "    reqbased=df[df['locality']==locality.lower()]\n",
    "    print(reqbased.head())\n",
    "    if budget!=None:\n",
    "        budget=int(budget)\n",
    "        new_budget=budget+(budget*0.10)\n",
    "        reqbased=reqbased[reqbased['price']<=(new_budget)]\n",
    "    #reqbased=reqbased.set_index(np.arange(reqbased.shape[0]))\n",
    "    print(get_index(reqbased,features))\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommedation_budget('Noida Extension' ,8000000, \"3 BHK with 1400 sqft apartment in Greater Noida West\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Society.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = re.findall(r'\\d+', \"3 BHK with minimum 1400 sqft apartment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in temp:\n",
    "    print(len(str(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizr=\"3 BHK with minimum 1400 sqft apartment\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esBigrams = ngrams(tokenizr, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esBigramFreq = collections.Counter(esBigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esBigramFreq.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].str.replace('[^\\w\\s]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_top_n_trigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-240ee1aebe31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcommon_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_top_n_trigram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_top_n_trigram' is not defined"
     ]
    }
   ],
   "source": [
    "common_names = get_top_n_trigram(df.index, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
